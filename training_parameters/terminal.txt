Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created
Agents initialized
Training started

Training on environment 1/2

Iteration 1/10 for environment 1
Collected 2 episodes
Model optimized: kl divergence = 4.723458668598154e-09
Policy improved: cost function = 2.8449838293989784

Stackelberg nash equilibrium reached after 1 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 2/2

Iteration 1/10 for environment 2
Collected 2 episodes
Model optimized: kl divergence = 9.360194598234477e-09
Policy improved: cost function = 2.583925946394051

Stackelberg nash equilibrium reached after 1 iterations

PS C:\Documenti\VS_Code\uni\AGT\stackelberg_MBRL>  c:; cd 'c:\Documenti\VS_Code\uni\AGT\stackelberg_MBRL'; & 'C:\Users\giuli\anaconda3\python.exe' 'c:\Users\giuli\.vscode\extensions\ms-python.python-2023.20.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher' '49272' '--' 'c:\Documenti\VS_Code\uni\AGT\stackelberg_MBRL\training_pal.py'
pygame 2.1.0 (SDL 2.0.16, Python 3.10.9)
Hello from the pygame community. https://www.pygame.org/contribute.html
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created
Agents initialized
Training started

Training on environment 1/20

Iteration 1/10 for environment 1
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 7354.046792842333

Iteration 2/10 for environment 1
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Negative kl divergence detected: kl divergence = -1.92583511384489e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0
C:\Users\giuli\anaconda3\lib\site-packages\nashpy\algorithms\support_enumeration.py:259: RuntimeWarning: 
An even number of (10) equilibria was returned. This
indicates that the game is degenerate. Consider using another algorithm
to investigate.

  warnings.warn(warning, RuntimeWarning)

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 2/20

Iteration 1/10 for environment 2
Collected 10 episodes
Model optimized: kl divergence = 3.2200116804858205e-16
Policy improved: cost function = 113489.00227133781

Iteration 2/10 for environment 2
Collected 10 episodes
Model optimized: kl divergence = 0.0010759596959290028
Policy improved: cost function = 6458.303966308644

Iteration 3/10 for environment 2
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Negative kl divergence detected: kl divergence = -1.6127823504772927e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Iteration 4/10 for environment 2
Collected 10 episodes
Model optimized: kl divergence = 0.000517231923071486
Policy improved: cost function = 0.0

Iteration 5/10 for environment 2
Collected 10 episodes
Model optimized: kl divergence = 0.0006330253982618996
Policy improved: cost function = 0.0

Iteration 6/10 for environment 2
Collected 10 episodes
Model optimized: kl divergence = 0.0002539060063693497
Policy improved: cost function = 10969.581585398864

Stackelberg nash equilibrium reached after 6 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 3/20

Iteration 1/10 for environment 3
Collected 10 episodes
c:\Documenti\VS_Code\uni\AGT\stackelberg_MBRL\src\algorithms\PAL.py:340: RuntimeWarning: invalid value encountered in divide
  q /= np.sum(q, axis=1)[:, None]
Model optimized: kl divergence = 0.0
Policy improved: cost function = 69077.53173536499

Iteration 2/10 for environment 3
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.5991357619615703e-06
Negative kl divergence detected: kl divergence = -1.677708335340847e-06
Negative kl divergence detected: kl divergence = -1.8315350959885272e-06
Negative kl divergence detected: kl divergence = -1.8926933896998059e-06
Negative kl divergence detected: kl divergence = -2.02991498854634e-06
Negative kl divergence detected: kl divergence = -2.026777727461924e-06
Model optimized: kl divergence = 0.0
Policy improved: cost function = 25018.46233967517

Iteration 3/10 for environment 3
Collected 10 episodes
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Negative kl divergence detected: kl divergence = -2.16813848436924e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 4/20

Iteration 1/10 for environment 4
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 93815.37889833104

Stackelberg nash equilibrium reached after 1 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 5/20

Iteration 1/10 for environment 5
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 39628.189715366105

Iteration 2/10 for environment 5
Collected 10 episodes
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Negative kl divergence detected: kl divergence = -2.7505818153123868e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 6/20

Iteration 1/10 for environment 6
Collected 10 episodes
Negative kl divergence detected: kl divergence = -5.0285149542408177e-26
Model optimized: kl divergence = 0.0
Policy improved: cost function = 106173.12579192081

Iteration 2/10 for environment 6
Collected 10 episodes
Model optimized: kl divergence = 0.0005170113475073145
Policy improved: cost function = 28593.962081001486

Iteration 3/10 for environment 6
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Negative kl divergence detected: kl divergence = -1.1014044027758284e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 7/20

Iteration 1/10 for environment 7
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 165221.51239904005

Iteration 2/10 for environment 7
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Negative kl divergence detected: kl divergence = -1.3084935532071856e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 8/20

Iteration 1/10 for environment 8
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 18676.60072287685

Iteration 2/10 for environment 8
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Negative kl divergence detected: kl divergence = -1.7730907113699477e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 9/20

Iteration 1/10 for environment 9
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 53527.943822108326

Iteration 2/10 for environment 9
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.6205913818846842e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Negative kl divergence detected: kl divergence = -1.840437525374814e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 10/20

Iteration 1/10 for environment 10
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 66171.46115085103

Iteration 2/10 for environment 10
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Negative kl divergence detected: kl divergence = -1.5713306475371653e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 11/20

Iteration 1/10 for environment 11
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.1102230246251568e-16
Model optimized: kl divergence = 0.0
Policy improved: cost function = 72963.73495110866

Iteration 2/10 for environment 11
Collected 10 episodes
Model optimized: kl divergence = 0.00015276707369028675
Policy improved: cost function = 5170.563739347521

Iteration 3/10 for environment 11
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Negative kl divergence detected: kl divergence = -1.7420171972357886e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 12/20

Iteration 1/10 for environment 12
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 76931.23247836396

Iteration 2/10 for environment 12
Collected 10 episodes
Model optimized: kl divergence = 0.00054206694517938
Policy improved: cost function = 6559.207612663819

Iteration 3/10 for environment 12
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Negative kl divergence detected: kl divergence = -1.2924891751396811e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 13/20

Iteration 1/10 for environment 13
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 111496.03906147108

Stackelberg nash equilibrium reached after 1 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 49)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 14/20

Iteration 1/10 for environment 14
Collected 10 episodes
Model optimized: kl divergence = 7.856129524859683e-10
Policy improved: cost function = 10583.58634752424

Iteration 2/10 for environment 14
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Negative kl divergence detected: kl divergence = -1.6786910274824012e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 2 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 15/20

Iteration 1/10 for environment 15
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 55008.631493788795

Iteration 2/10 for environment 15
Collected 10 episodes
Model optimized: kl divergence = 2.6685136135591382e-09
Policy improved: cost function = 0.0

Iteration 3/10 for environment 15
Collected 10 episodes
Model optimized: kl divergence = 0.0008120151479883014
Policy improved: cost function = 6903.6058743919175

Iteration 4/10 for environment 15
Collected 10 episodes
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Negative kl divergence detected: kl divergence = -9.981118170719705e-18
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 4 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 16/20

Iteration 1/10 for environment 16
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 62000.30686774821

Iteration 2/10 for environment 16
Collected 10 episodes
Model optimized: kl divergence = 0.0011439853458310857
Policy improved: cost function = 19229.458481819656

Iteration 3/10 for environment 16
Collected 10 episodes
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Negative kl divergence detected: kl divergence = -2.5189317653593214e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 17/20

Iteration 1/10 for environment 17
Collected 10 episodes
Model optimized: kl divergence = 8.674847822601629e-05
Policy improved: cost function = 79086.27239777685

Iteration 2/10 for environment 17
Collected 10 episodes
Model optimized: kl divergence = 0.0002593915450955638
Policy improved: cost function = 6645.68796982669

Iteration 3/10 for environment 17
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.584999779078642e-17
Negative kl divergence detected: kl divergence = -1.584999779078642e-17
Negative kl divergence detected: kl divergence = -1.584999779078642e-17
Negative kl divergence detected: kl divergence = -1.584999779078642e-17
Negative kl divergence detected: kl divergence = -1.584999779078642e-17
Negative kl divergence detected: kl divergence = -1.584999779078642e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 18/20

Iteration 1/10 for environment 18
Collected 10 episodes
Model optimized: kl divergence = 0.0
Policy improved: cost function = 72625.0572905395

Iteration 2/10 for environment 18
Collected 10 episodes
Model optimized: kl divergence = 0.0007637760761174072
Policy improved: cost function = 7502.7597108595955

Iteration 3/10 for environment 18
Collected 10 episodes
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Negative kl divergence detected: kl divergence = -1.4130186916189215e-17
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (99, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 19/20

Iteration 1/10 for environment 19
Collected 10 episodes
Model optimized: kl divergence = 0.004149401407347403
Policy improved: cost function = 44676.22406779343

Iteration 2/10 for environment 19
Collected 10 episodes
Model optimized: kl divergence = 3.690409134656388e-05
Policy improved: cost function = 11368.259356190683

Iteration 3/10 for environment 19
Collected 10 episodes
Negative kl divergence detected: kl divergence = -8.861447040613625e-18
Negative kl divergence detected: kl divergence = -8.861447040613625e-18
Negative kl divergence detected: kl divergence = -8.861447040613625e-18
Negative kl divergence detected: kl divergence = -8.861447040613625e-18
Model optimized: kl divergence = 0.0
Policy improved: cost function = 0.0

Stackelberg nash equilibrium reached after 3 iterations

Reset environment and initialize a new one
Initialize maze
Maze created
Maze size: 101x51
Compute initial and terminal state
Initial state: (1, 1)
Compute prior and transitional distribuitions
Compute reward function
Environment created

Training on environment 20/20

Iteration 1/10 for environment 20
Collected 10 episodes
Model optimized: kl divergence = 1.3349244430451828e-11
Policy improved: cost function = 48773.68581306693

Iteration 2/10 for environment 20
Collected 10 episodes
Model optimized: kl divergence = 4.894734315145979e-05
Policy improved: cost function = 97769.8146080582

Stackelberg nash equilibrium reached after 2 iterations